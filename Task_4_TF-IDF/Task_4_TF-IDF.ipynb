{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b847d600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/monitor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c0208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, определяющая, является ли строка пустой\n",
    "def isBlank (string):\n",
    "    if string and string.strip():\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8841049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "patterns = \"[A-Za-z0-9(){}–«»,:;\\\"'.“”!#$%&<=>?@\\xa0/№-]+\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "# В цикле читаются все документы из папки ./documents, из текста каждого документа убираются спец. символы,\n",
    "# указанные в переменной patterns\n",
    "# После этого тексты всех документов собирается в один массив\n",
    "for i in range(1,101):\n",
    "    path='./documents/out{num}.txt'.format(num=i)\n",
    "    with open(path,'r') as file:\n",
    "        text = ''\n",
    "        for line in file.readlines():\n",
    "            if not isBlank(line):\n",
    "                line = re.sub(patterns, ' ', line)\n",
    "                text += ' ' + line\n",
    "        text = text.replace('\\n', ' ')\n",
    "        texts.append(text)\n",
    "\n",
    "# print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c8606ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8658   5314\n"
     ]
    }
   ],
   "source": [
    "tokens = set()\n",
    "lemmas = set()\n",
    "\n",
    "# чтение терминов из файла с терминами\n",
    "with open('./tokens.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        tokens.add(line.strip())\n",
    "\n",
    "# чтение лемм из файла с леммами    \n",
    "with open('./lemmas.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        lemmas.add(line.split(\":\")[0].strip())\n",
    "    \n",
    "print(len(tokens), ' ', len(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27862b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "# функция для подсчета tf для терминов и лемм\n",
    "# в цикле идет проход по текстам всех документов, для каждого документа в два словаря \n",
    "# добавляется или обновляется информация о каждом термине и каждой лемме(номер документа и tf в документе)\n",
    "\n",
    "\n",
    "def count_tf(texts):\n",
    "    lms_tf = {}\n",
    "    tks_tf = {}\n",
    "    for i in range(0,len(texts)):\n",
    "        words = texts[i].split(' ')\n",
    "        tklen = 0\n",
    "        for word in words:\n",
    "            token = word.lower()\n",
    "            if token and token not in stopwords_ru:\n",
    "                token = token.strip()\n",
    "                \n",
    "                if token in tokens:\n",
    "                    if token in tks_tf.keys():\n",
    "                        if i+1 in tks_tf[token].keys():\n",
    "                            tks_tf[token][i+1] += 1\n",
    "                        else:\n",
    "                            tks_tf[token][i+1] = 1\n",
    "                    else:\n",
    "                        tks_tf[token] = {i+1:1}\n",
    "                    tklen += 1\n",
    "                \n",
    "                \n",
    "                nor_token = morph.normal_forms(token)[0]\n",
    "                if nor_token in lemmas:\n",
    "                    if nor_token in lms_tf.keys():\n",
    "                        if i+1 in lms_tf[nor_token].keys():\n",
    "                            lms_tf[nor_token][i+1] += 1\n",
    "                        else:\n",
    "                            lms_tf[nor_token][i+1] = 1\n",
    "                    else:\n",
    "                        lms_tf[nor_token] = {i+1:1}\n",
    "        \n",
    "        for t in tks_tf.keys():\n",
    "            if i+1 in tks_tf[t].keys():\n",
    "                tks_tf[t][i+1] /= len(words)\n",
    "        \n",
    "        for l in lms_tf.keys():\n",
    "            if i+1 in lms_tf[l].keys():\n",
    "                lms_tf[l][i+1] /= tklen\n",
    "    \n",
    "    return tks_tf,lms_tf\n",
    "                    \n",
    "                \n",
    "tks_tf,lms_tf = count_tf(texts)\n",
    "\n",
    "# tks_tf,lms_tf - словари формата \n",
    "# {слово: {номер_документа: tf, номер_документа: tf, ...}, \n",
    "# слово: {номер_документа: tf, номер_документа: tf, ...} ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "793ce0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# получение данных для результирующих файлов\n",
    "# для каждого слова в словарях tks_tf,lms_tf в результирующий словарь \n",
    "# по ключу документа добавляется информация вида слово: [tf, tf*idf]\n",
    "# tks_res, lms_res - словари вида \n",
    "# {номер_документа: {слово:[idf, tf*idf], слово:[idf, tf*idf], ...},\n",
    "# номер_документа: {слово:[idf, tf*idf], слово:[idf, tf*idf], ...} ...}\n",
    "\n",
    "tks_res = {}\n",
    "\n",
    "for key in tks_tf.keys():\n",
    "    idf = math.log10(100/len(tks_tf[key].keys()))\n",
    "    \n",
    "    for k in tks_tf[key].keys():\n",
    "        if k in tks_res.keys():\n",
    "            tks_res[k][key] = [idf, idf*tks_tf[key][k]]\n",
    "        else:\n",
    "            tks_res[k] = {key: [idf, idf*tks_tf[key][k]]}\n",
    "\n",
    "lms_res = {}\n",
    "\n",
    "for key in lms_tf.keys():\n",
    "    idf = math.log10(100/len(lms_tf[key].keys()))\n",
    "    \n",
    "    for k in lms_tf[key].keys():\n",
    "        if k in lms_res.keys():\n",
    "            lms_res[k][key] = [idf, idf*lms_tf[key][k]]\n",
    "        else:\n",
    "            lms_res[k] = {key: [idf, idf*lms_tf[key][k]]}\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ff80afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запись выходных данных в файлы\n",
    "\n",
    "for num in tks_res.keys():\n",
    "    with open('./result_tokens/outt{}.txt'.format(num),'w') as t_file:\n",
    "        for word in tks_res[num].keys():\n",
    "            t_file.write('{} {} {}\\n'.format(word,tks_res[num][word][0],tks_res[num][word][1]))\n",
    "    \n",
    "    with open('./result_lemmas/outl{}.txt'.format(num),'w') as l_file:\n",
    "        for word in lms_res[num].keys():\n",
    "            l_file.write('{} {} {}\\n'.format(word,lms_res[num][word][0],lms_res[num][word][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d1c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
